**ğŸ—“ Day - 11 :Deep Learning & Neural NetworksğŸš€**

Date: 3/06/2025

**ğŸ“Œ What is Deep Learning?**

![WhatsApp Image 2025-06-04 at 7 08 01 AM (2)](https://github.com/user-attachments/assets/99e3dd76-82ef-40df-99c5-64607c2a321d)

Deep Learning is a subset of Machine Learning that uses neural networks with multiple layers (deep neural networks) to model complex patterns in large datasets. It is especially useful for tasks such as image recognition, speech processing, and natural language understanding.

**ğŸ¤” Why Deep Learning?**

![WhatsApp Image 2025-06-04 at 7 08 01 AM (1)](https://github.com/user-attachments/assets/7c1c1c1c-84d8-4ccb-b970-6ce958054721)

Capable of automatically learning features from raw data.

Performs well on unstructured data (images, audio, text).

Outperforms traditional ML in complex problem domains.

Enables end-to-end learning without manual feature engineering.

**ğŸ†š Machine Learning vs Deep Learning**

| Aspect              | Machine Learning       | Deep Learning                         |
| ------------------- | ---------------------- | ------------------------------------- |
| Feature Engineering | Manual                 | Automatic                             |
| Data Requirement    | Less                   | Large volumes required                |
| Interpretability    | Easier to interpret    | More complex, harder to interpret     |
| Performance         | Good for simpler tasks | Better for complex, large-scale tasks |


**ğŸ§© Deep Neural Networks (DNN)**

DNNs consist of multiple layers of artificial neurons, with each layer transforming its input data to a slightly more abstract representation.

**ğŸ§± Representation of DNN**

A DNN has:

Input layer: Accepts the features.

Hidden layers: One or more layers that transform data.

Output layer: Produces final prediction/classification.

**ğŸ§  Neurons & Artificial Neural Networks (ANN)**

ğŸ§¬ Components of an Artificial Neuron

Inputs (xâ‚, xâ‚‚, ..., xâ‚™)

Weights (wâ‚, wâ‚‚, ..., wâ‚™)

Bias (b)

Activation function (Ïƒ)

Output: y = Ïƒ(wâ‚xâ‚ + wâ‚‚xâ‚‚ + ... + wâ‚™xâ‚™ + b)

**ğŸ”¹ Single Neuron**

Acts as a basic unit of computation, similar to a perceptron. It applies weights and bias to input features, then passes it through an activation function.

**âš™ï¸ Activation Functions**

Introduce non-linearity to the model:

ReLU (Rectified Linear Unit)

Sigmoid

Tanh

Softmax (for multiclass classification)

**ğŸ” Training a Neural Network**

**â–¶ï¸ Forward Propagation**

Input is passed through each layer

Weighted sum is computed and passed through activation functions

Output is generated

**â®ï¸ Backward Propagation**

Loss is calculated by comparing prediction to actual output

Gradients are computed using chain rule

Weights are updated using gradient descent

**âŒ› Epochs & Batch Size**

Epoch: One complete pass through the entire dataset.

Batch Size: Number of training examples processed before the model updates weights.

**âš™ï¸ Hyperparameters in Neural Networks**

**ğŸ”§ Network Structure**

Number of Hidden Layers and Neurons: Controls model capacity.

Dropout: Prevents overfitting by randomly disabling neurons during training.

Activation Functions: Adds non-linearity and affects learning behavior.

**ğŸ§ª Training Algorithm**

Learning Rate: Controls how much weights are adjusted.

Loss Functions:

MSE (Mean Squared Error) â€“ Regression tasks

Cross-Entropy â€“ Classification tasks

**ğŸ”„ Multilayer Perceptron (MLP)**

A fully connected feedforward neural network:

Contains multiple layers of neurons.

Each neuron in one layer is connected to every neuron in the next.

Ideal for structured data tasks.

